
\subsection{Modules and infinitesimal disks}
The most basic infinitesimal schemes are the first order neighbourhoods in affine n-space $R^n$. Their algebra of functions is $R^{n+1}$, which is an instance of the more general construction below.

For any $R$-module $M$, there is an $R$-algebra structure on $R\oplus M$ with multiplication given by
\[(r,m)(r',m') = (rr',rm'+r'm)\]
Algebras of this form are called \emph{square zero extensions} of $R$, since products of the form $(0,m)(0,n)$ are zero.
By this property, for any $R$-linear map $\varphi:M\to N$ between modules $M,N$, the map $\mathrm{id}\oplus \varphi: R\oplus M\to R\oplus N$ is an $R$-algebra homomorphism. In particular, if $M$ is finitely presented, i.e.\ merely the cokernel of some $p:R^n\to R^m$ then $R\oplus M$ is the cokernel of a map between finitely presented algebras and therefore finitely presetend as an algebra. 

\begin{definition}
Given $M$ a finitely presented $R$-module, we define an f.p.\ algebra structure on $R\oplus M$ as above and set:
\[\D(M)\colonequiv \Spec(R\oplus M)\]
This is a pointed scheme by the first projection which we denote $0$
and the construction is functorial by the discussion above.
\end{definition}

We write $\D(n)$ for $\D(R^n)$ so that for example:
\[\D(1) = \Spec(R[X]/(X^2)) = \{\epsilon:R\ |\ \epsilon^2=0\}\]

\begin{definition}
Assume given $M$ an f.p.\ $R$-module and $A$ an f.p.\ $R$-algebra with $x:\Spec(A)$. An $M$-derivation at $x$ is a morphism of $R$-modules:
\[d:A\to M\]
such that for all $a,b:A$ we have that:
\[d(ab) = a(x)d(b) + b(x)d(a)\]
\end{definition}

\begin{lemma}\label{tangent-are-derivation}
Assume given $M$ an f.p.\ module and $A$ an f.p.\ algebra with $x:\Spec(A)$. Pointed maps:
\[\D(M)\to_\pt (\Spec(A),x)\] 
corresponds to $M$-derivation at $x$.
\end{lemma}

\begin{proof}
Such a pointed map correponds to an algebra map:
\[f : A\to R\oplus M\]
where the composite with the first projection is $x$. This means that, for some module map $d:A\to M$ we have:
\[f(a) = (a(x),d(a))\]
We can immediately see that $f$ being a map of $R$-algebras is equivalent to $d$ being an $M$-derivation at $x$.
\end{proof}

\begin{lemma}\label{equivalence-module-infinitesimal}
Let $M$, $N$ be finitely presented modules. Then linear maps $M \to N$ correspond to
pointed maps $\D(N) \to_\pt \D(M)$. 
%Explicitly, a linear map $g : M \to N$ corresponds to the pointed map $f \mapsto m \mapsto f(g(m))$.
\end{lemma}

\begin{proof}
By \cref{tangent-are-derivation} such a pointed map corresponds to an $N$-derivation at $0:\D(M)$.

Such a derivation is a morphism of modules:
\[d:R\oplus M\to N\]
such that for all $(r,m),(r',m'):R\oplus M$ we have that:
\[d(rr',rm'+r'm) = rd(r',m')+r'd(r,m)\]
This implies $d(r,0) = 0$ for all $r : R$, so we obtain a section to the injective functorial action of $\mathbb{D}$ on linear maps.
\end{proof}


\subsection{Tangent spaces}

\begin{definition}
Let $X$ be a type and let $x:X$, then we define the tangent space $T_x(X)$ of $X$ at $x$ by:
\[\{t:\D(1)\to X\ |\ t(0)=x\}\]
\end{definition}

\begin{definition}
Given $f:X\to Y$ and $x:X$ we have a map:
\[df_x : T_x(X)\to T_{f(x)}(Y)\]
induced by post-composition.
\end{definition}

\begin{lemma}\label{An-dimension-n}
For all $x:R^n$ we have $T_x(R^n) = R^n$.
\end{lemma}

\begin{proof}
Since $R^n$ is homogeneous we can assume $x=0$. By \cref{tangent-are-derivation} we know that $T_0(R^n)$ corresponds to the type of linear maps
\[R[X_1,\cdots,X_n] \to R\]
such that for all $P,Q$ we have:
\[d(PQ) = P(0)dQ + Q(0)dP\]
which is equivalent to $d(1) = 0$ and $d(X_iX_j) = 0$, so any such map is determined by its image on the $X_i$ so it is equivalent to an element of $R^n$.
\end{proof}

\begin{lemma}\label{from-D1-to-D2}
Given a scheme $X$ with $x:X$ and $v,w:T_x(X)$, there exists a unique:
\[\psi_{v,w} : \D(2)\to_\pt X\]
such that for all $\epsilon:\D(1)$ we have that:
\[\psi_{v,w}(\epsilon,0) = v(\epsilon)\]
\[\psi_{v,w}(0,\epsilon) = w(\epsilon)\]
\end{lemma}

\begin{proof}
We can assume $X$ is affine. Then $\D(2)\to_\pt X$ is equivalent to the type of $R^2$-derivations at $x$, but giving an $M\oplus N$-derivation is equivalent to giving an $M$-derivation and an $N$-derivation. Checking the equalities is a routine computation.
\end{proof}

\begin{lemma}
For any scheme $X$ and $x:X$, we have that $T_x(X)$ is a module.
\end{lemma}

\begin{proof}
There is a more conceptual proof given as \cite{david-orbifolds}[Theorem 4.2.19] which could be made to work with schemes -- we proceed by sketching a more tedious, explicit proof with less technical prerequisites.
We define scalar multiplication by sending $v$ to $t\mapsto v(rt)$.

Then for addition of $v,w:T_x(X)$, we define:
\[(v+w)(\epsilon) = \psi_{v,w}(\epsilon,\epsilon)\]
where $\psi_{v,w}$ is defined in \cref{from-D1-to-D2}.

We omit checking that this is a module structure.
\end{proof}

\begin{lemma}
For $f:X\to Y$ a map between schemes, for all $x:X$ the map $df_x$ is a map of $R$-modules.
\end{lemma}

\begin{proof}
Commutation with scalar multiplication is immediate.

Commutation with addition comes by applying uniqueness in \cref{from-D1-to-D2} to show:
\[f\circ \psi_{v,w} = \psi_{f\circ v,f\circ w}\]
\end{proof}

\begin{lemma}\label{kernel-is-tangent-of-fibers}
For any map $f:X\to Y$ and $x:X$, we have that:
\[
\mathrm{Ker}(df_x) = T_{(x,\refl_{f(x)})}(\mathrm{fib}_f(f(x)))
\]
\end{lemma}

\begin{proof}
This holds because:
\[
(\mathrm{fib}_f(f(x)),(x,\refl_{f(x)}))
\]
is the pullback of:
\[
(X,x) \to (Y,f(x)) \leftarrow (1,*)
\]
in pointed types, applied using $(\mathbb{D}(1),0)$.
\end{proof}

\begin{lemma}\label{tangent-finite-copresented}
Let $X$ be a scheme with $x : X$. Then $T_x(X)$ is a finitely
co-presented $R$-module.
\end{lemma}

\begin{proof}
We cab assume $X$ affine. Then $X$ is the kernel of a map:
\[P:R^m\to R^n\]
so that for all $x:X$, by applying \cref{kernel-is-tangent-of-fibers} we know that we have $T_x(X)$ is the kernel of:
\[dP_x : T_x(R^m)\to T_0(R^n)\]
and we conclude by \cref{An-dimension-n}.
\end{proof}

\begin{corollary}
  \label{tangent-bundle-scheme}
  Let $X$ be a scheme, then the tangent bundle $X^{\mathbb{D}(1)}$ is a scheme.
\end{corollary}

\begin{proof}
  We give two independent arguments, the first uses the lemma, the second is a direct computation:
  \begin{enumerate}[(i)]
  \item Finitely co-presented modules are schemes, since they are the common zeros of linear functions on $R^n$.
    So by the lemma, all tangent spaces $T_x(X)$ are schemes and
    \[
      X^{\mathbb{D}(1)}=\sum_{x:X}T_x(X)
    \]
    is a dependent sum of schemes and therefore a scheme.
  \item Let $X$ be covered by open affine $U_1,\dots,U_n$ then $U_1^{\mathbb{D}(1)},\dots,U_n^{\mathbb{D}(1)}$ is an open cover by double negation stability of opens.
    So we conclude by showing that for affine $Y=\Spec R[X_1,\dots,X_n]/(f_1,\dots,f_l)$ the tangent bundle $Y^{\mathbb{D}(1)}$ is affine
    by direct computation:
    \begin{align*}
      Y^{\mathbb{D}(1)}&=\Hom_{\Alg{R}}(R[X_1,\dots,X_n]/(f_1,\dots,f_l), R\oplus \epsilon R) \\
                       &= \{(y_1,\dots,y_n):R\oplus \epsilon R \mid \forall_i. f_i(y_1,\dots,y_n)=0\} \\
                       &= \{(x_1,\dots,x_n,d_1,\dots,d_n):R^{2n} \mid \forall_i. f_i(x_1,\dots,x_n)=0\text{ and } \sum_j d_j\frac{\partial f_i}{\partial X_j}(x_1,\dots,x_n) =0\} 
    \end{align*}
  \end{enumerate}
\end{proof}

\subsection{Infinitesimal neighbourhoods}

\begin{definition}
Let $X$ be a set with $x:X$. The first order neighborhood $N_1(x)$ is defined as the set of $y:X$ such that there exists an f.g.\ ideal $I\subseteq R$ with $I^{2}=0$ and:
\[I=0 \to x=y\]
\end{definition}

\begin{lemma}\label{first-order-square-zero}
Assume $x,y:R^n$, then $x\in N_1(y)$ if and only if the ideal generated by the $x_i-y_i$ has square zero.
\end{lemma}

\begin{proof}
Let us denote $I$ the ideal generated by the $x_i-y_i$ so that we clearly have $x=y$ if and only if $I=0$. 

If $I^2=0$ then it is clear that $y\in N_1(x)$.

Conversely if $J=0 \to I=0$ then we have that $I\subset J$ by duality so that $J^2=0$ implies $I^2=0$.
\end{proof}

\begin{lemma}\label{first-order-schemes}
Let $X$ be a scheme with $x:X$. Then $N_1(x)$ is an affine scheme.
\end{lemma}

\begin{proof}
If $x\in U$ open in $X$, we have that $N_1(x)\subset U$ so that we can assume $X$ affine.

This means $X$ is a closed subscheme $C\subset R^n$. Then by \cref{first-order-square-zero}, we have that $N_1(x)$ is the type of $y:R^n$ such that $y\in C$ and for all $i,j$ we have that $(x_i-y_i)(x_j-y_j) = 0$, which is a closed subset of $C$ so it is an affine scheme.
\end{proof}

\begin{definition}
A pointed scheme $(X,*)$ is called a \notion{first order (infinitesimal) disk} if for all $x:X$ we have $x\in N_1(*)$.
\end{definition}

\begin{lemma}\label{disk-are-infinitesimal}
A pointed scheme $(X,*)$ is a first order disk if and only if there exists a finitely presented module $M$ such that:
\[(X,*) = (\D(M),0)\]
\end{lemma}

\begin{proof}
First we check that for all $M$ finitely presented and $y:\D(M)$ we have that $y\in N_1(0)$. Let $m_1,\cdots, m_n$ be generators of $M$, then consider $d:M\to R$ induced by $y$, then $y=0$ if and only if $d=0$ and for all $i,j$ we have that:
\[d(m_i)d(m_j) = 0\]
This means that $I = (d(m_1),\cdots,d(m_n))$ has square $0$ and $I=0$ implies $y=0$ so that $y\in N_1(0)$.

For the converse we assume $X$ a first order disk, by \cref{first-order-schemes} we have that $X$ is affine and pointed, up to translation we can assume $X\subset R^n$ closed pointed by $0$. Since $X$ is a first order disk we have that $X\subset N_1(0)$ and by \cref{first-order-square-zero} we have $N_1(0) = \D(R^n)$.

This means there is an f.g.\ ideal $J$ in $R\oplus R^n$ such that $X=\Spec(R\oplus R^n / J)$.
But $0\in X$ corresponds to the first projection from $R\oplus R^n$ -- meaning if $(x,y)\in J$ then $x=0$, so that $J$ corresponds uniquely to an f.g.\ sub-module $K$ of $R^n$ and:
\[X = \Spec(R\oplus (R^n/K)) = \D(R^n/K)\] 
\end{proof}

\begin{definition}
  Let $M^\star\colonequiv \Hom_{R}(M,R)$ denote the $R$-linear dual of an $R$-module $M$.
\end{definition}

While it is clear that the dual of a finite presentation yields a finite co-presentation, the reverse is not true in general, but holds with the duality axiom.
We will give a proof of this fact in \Cref{dual-of-fcop-fp}, which will need the following two extension results.

\begin{lemma}
  \label{extend-from-kernel}
  Let $M\subseteq R^n$ be the kernel of a linear map between finite free $R$-modules.
  Then any linear map $M\to R$ can be extended to $R^n$.
\end{lemma}

\begin{proof}
  First note that $M=\Spec(R[X_1,\dots,X_n]/(l_1,\dots,l_m))$ is affine, where the $l_i$ are linear.
  Let $L:M\to R$ be $R$-linear and $P:R^n\to R$ be given by taking a preimage under the quotient map $R[X_1,\dots,X_n]\to R[X_1,\dots,X_n]/(l_1,\dots,l_m)$,
  so we have $P_{\vert M}=L$.
  Let $P=\sum_{\sigma:\N^{\{1,\dots,n\}}}a_\sigma X_1^{\sigma(1)}\cdots X_n^{\sigma(n)}$.
  Now we can conclude by showing that the linear part of $P$
  \[
    K\colonequiv \sum_{\sigma:\N^{\{1,\dots,n\}}, \sum \sigma =1}a_\sigma X_1^{\sigma(1)}\cdots X_n^{\sigma(n)}
  \]
  extends $L$ as well, i.e.\ we will see $K_{\vert M}=L$.
  
  For all $x:M$ and $\lambda : R$ we have $L(\lambda x)=\lambda L(x)$ and therefore
  \[
    \sum_{\sigma:\N^{\{1,\dots,n\}}}\lambda^{\sum\sigma} a_\sigma x_1^{\sigma(1)}\cdots x_n^{\sigma(n)}=\lambda \sum_{\sigma:\N^{\{1,\dots,n\}}} a_\sigma x_1^{\sigma(1)}\cdots x_n^{\sigma(n)}
  \]
  By comparing coefficients as polynomials in $\lambda$, we have $\sum_{\sigma:\N^{\{1,\dots,n\}}, \sum \sigma \neq 1}a_\sigma x_1^{\sigma(1)}\cdots x_n^{\sigma(n)}=0$,
  which shows $K_{\vert M}=P_{\vert M}=L$.
\end{proof}

\begin{lemma}
  \label{extend-from-image}
  Let $\varphi:R^n\to R^m$ be $R$-linear, then any linear map $\mathrm{im}(\varphi)\to R$ on the image of $\varphi$ can be extended to $R^m$.
\end{lemma}

\begin{proof}\footnote{This proof is due to Thierry Coquand.}
  Let $(a_{ij})_{ij}$ be the coefficients of the matrix representing  $\varphi$ with respect to the standard basis.
  Then the image of $\varphi$ is generated by the columns of this matrix:
  \[
    \mathrm{im}(\varphi)=\left\{\sum_{j=1}^n x_j(a_{\_j})\mid x_j:R\text{, $1\leq j \leq n$}\right\}
  \]
  Let $L:\mathrm{im}(\varphi)\to R$ be $R$-linear and $l_j\colonequiv L((a_{\_j}))$.
  Applying $L$ to a general element of $\mathrm{im}(\varphi)$ and using linearity yields the following implication:
  \[
    \sum_{j=1}^n x_j(a_{\_j}) = 0 \Rightarrow \sum_{j=1}^n x_jl_j = 0
  \]
  The left side being 0 means that $m$ linear polynomials $P_i(x_1,\dots,x_n)\colonequiv\sum_{j=1}^n x_ja_{ij} $ vanish simulataneously.
  Let $Q(x_1,\dots,x_n)$ be the linear polynomial on the right side of the implication.
  Then the implication induces an inclusion between the common zeros of the $P_i$ and the zeros of $Q$,
  which by duality means that we have an inclusion of ideals $(Q)\subseteq (P_1,\dots,P_m)$ in $R[X_1,\dots,X_n]$.
  So there $b_i:R[X_1,\dots,X_n]$ such that
  \[
     Q = \sum_{i=1}^m b_iP_i
   \]
   By comparing coefficients it is clear that the $b_i$ can be chosen to be in $R$, which we assume now.
   We define a $R$-linear map $K:R^m\to R$ by $K((y_1,\dots,y_m)^T)\colonequiv\sum_{i=1}^m b_iy_i$.
   $K$ extends $L$:
   \begin{align*}
     K\left(\sum_{j=1}^n x_j(a_{\_j})\right)=&\sum_{i=1}^m b_i\sum_{j=1}^n x_ja_{ij} \\
     =& \sum_{i=1}^m b_iP_i(x_1,\dots,x_n) \\
     =& Q(x_1,\dots,x_n) \\
     =& \sum_{j=1}^n x_jl_j \\
     =& L\left(\sum_{j=1}^n x_j(a_{\_j})\right)
   \end{align*}
\end{proof}

\begin{lemma}
  \label{dual-of-fcop-fp}
  Let $M$ be finitely co-presented, i.e.\ let there be an exact sequence
  \begin{center}
    \begin{tikzcd}
      M\ar[r,hook, "\varphi"] & R^n\ar[r] & R^m
    \end{tikzcd}
  \end{center}
  Then the dual of this sequence is exact as well and $\varphi^\star$ is surjective.
  In particular, $M^\star$ is finitely presented.
\end{lemma}

\begin{proof}
  Surjectivity of $\varphi^\star$ follows from \Cref{extend-from-kernel}.
  Linear maps $R^n\to R$ which vanish on $M$ factor over the image of $P$, so exactness at the middle of the dual sequence follows from \Cref{extend-from-image}.
\end{proof}

\begin{lemma}\label{equivalence-modules-disks}
  The functor from finitely copresented modules to first order disks:
  \[M\mapsto \D(M^\star)\]
  is an equivalence, with inverse:
  \[(X,x)\mapsto T_x(X)\]
\end{lemma}

\begin{proof}
It is fully faithful by \cref{equivalence-module-infinitesimal} and essentially surjective by \cref{disk-are-infinitesimal}. To check for the inverse it is enough to check that:
\[T_0(\D(M^\star)) = M\]
which is also a consequence of \cref{equivalence-module-infinitesimal}.
\end{proof}

\begin{lemma}\label{duality-infinitesimal-tangent}
Let $X$ be a scheme with $x:X$, then we have:
\[N_1(x) = \D(T_x(X)^\star)\]
\end{lemma}

\begin{proof}
By \cref{first-order-schemes} we have that $(N_1(x),x)$ is a first order disk. By \cref{equivalence-modules-disks} it is enough to check that $T_x(N_1(x)) = T_0(\D(T_x(X)^\star))$. 

It is immediate that any map $f:\D(1)\to X$ uniquely factors through $N_1(f(0))$ so that $T_x(N_1(x)) = T_x(X)$, and we have that $T_0(\D(T_x(X)^\star)) = T_x(X)$ by \cref{equivalence-modules-disks}.
\end{proof}


\subsection{Projectivity of finitely copresented modules}

\begin{lemma}\label{tangent-copresented-modules}
Let $M$ be a finitely copresented module, then we have $T_0(M) = M$.
\end{lemma}

\begin{proof}
We have that $M$ is the kernel of a linear map $P:R^m\to R^n$. By \cref{kernel-is-tangent-of-fibers} we have that $T_0(M)$ is the kernel of:
\[dP_0:T_0(R^m)\to T_0(R^n)\]
but by \cref{An-dimension-n} this is a map from $R^m$ to $R^n$, we omit the verification that $dP_0 = P$.
\end{proof}

\begin{lemma}
Any finitely copresented module is projective.
\end{lemma}

\begin{proof}
We consider $M,N$ finitely copresented with a surjective map:
\[f:M\to N\]
By \cref{duality-infinitesimal-tangent} and \cref{tangent-copresented-modules} we know that $\D(M^*) = N_1(0)$ in $M$, so that we have a commutative diagram:
\begin{center}
\begin{tikzcd}
\D(M^\star)\ar[r]\ar[d] & M\ar[d,"f"]\\
\D(N^\star)\ar[r,"i"] & N\\
\end{tikzcd}
\end{center}
Since $\D(N^*)$ has choice and $f$ is surjective there is $g:\D(N^\star)\to M$ such that $f\circ g = i$. Up to translation we can assume $g(0) = 0$.
Then we can factor $g$ through $\D(M^\star)$ as $N_1$ is functorial. This gives us a pointed section of the map:
\[\D(M^\star) \to \D(N^\star)\]
which by \cref{equivalence-module-infinitesimal} gives a linear section of $f$. (TODO should we check functoriality?)
\end{proof}

\begin{lemma}\label{neighborhood-tangent-correspondence-smooth}
A linear map between finitely copresented module:
\[f:M\to N\]
is surjective if and only if the corresponding pointed map:
\[\D(M^\star) \to \D(N^\star)\]
merely has a section preserving $0$.
\end{lemma}

\begin{proof}
By \cref{equivalence-module-infinitesimal} we know that:
\[\D(M^\star) \to \D(N^\star)\]
merely having a section preserving $0$ is equivalent to:
\[f:M\to N\]
merely having a section. But since any finitely copresented module is projective, this is equivalent to $f$ being surjective.
\end{proof}


\subsection{Rank of matrices}

\begin{definition}
A matrix is said to have rank $\leq n$ all its $n+1$-minors are zero. It is said to have rank $n$ if it has rank $\leq n$ and does not have rank $\leq n-1$.
\end{definition}

Having a rank is a property of matrices, as a rank function defined on all matrices would allow to e.g. decide if an $r:R$ is invertible.

\begin{lemma}\label{rank-bloc-matrix}
Assume given a matrix $M$ of rank $n$ decomposed into blocks:
\[M = \begin{pmatrix}
P & Q  \\
R & S \\
\end{pmatrix}\]
Such that $P$ is square of size $n$ and invertible. Then we have:
\[S = RP^{-1}Q\]
\end{lemma}

\begin{proof}
By columns manipulation the matrix is equivalent to:
\[M = \begin{pmatrix}
P & 0  \\
0 & S - RP^{-1}Q \\
\end{pmatrix}\]
but equivalent matrices have the same rank so $S=RP^{-1}Q$.
\end{proof}

\begin{lemma}\label{rank-equivalent-definitions}
If a linear map:
\[M:R^m \to R^n\]
has kernel $R^k$, then it has rank $m-k$.
\end{lemma}

\begin{proof}
Let $a_1,\cdots,a_n$ be a basis for the kernel of $M$ in $R^m$, which we complete into a basis of $R^m$ via $b_{n+1},\cdots,b_m$. By completing $f(b_{n+1}),\cdots, f(b_m)$ to a basis of $R^n$, we get a basis where $M$ is written as:
\[\begin{pmatrix}
I_n & (0)  \\
(0) & (0) \\
\end{pmatrix}\]
so that $M$ has rank $n$.
\end{proof}

%\begin{definition}
%Two matrices $M,N$ are said equivalent if there are invertible matrices $P,Q$ such that $M = PNQ$.
%\end{definition}

%It is clear that equivalent matrices have the same rank.

%\begin{lemma}\label{rank-equivalent-definitions}
%Assume given a matrix:
%\[M : R^m\to R^k\]
%Then the following are equivalent:
%\begin{enumerate}[(i)]
%\item $M$ has rank $n$.
%\item The kernel of $M$ is equivalent to $R^{m-n}$.
%\item The image of $M$ is equivalent to $R^n$.
%\item $M$ is equivalent to the bloc matrix:
%\[\begin{pmatrix}
%I_n & (0)  \\
%(0) & (0) \\
%\end{pmatrix}\]
%\end{enumerate}
%\end{lemma}

%\begin{proof}
%\end{proof}





